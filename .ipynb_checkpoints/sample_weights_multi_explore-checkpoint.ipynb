{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = '/home/drevital/obstacles_classification_datasets/rgb_6_balanced'\n",
    "out_dir = '/home/drevital/obstacles_classification_datasets/shufersal_sep22_d_2__1_5'\n",
    "sites_dir = '/home/drevital/obstacles_classification_datasets/rgb_6_balanced/sites'\n",
    "sites = os.listdir(sites_dir)\n",
    "\n",
    "# Parameters used in the diff_metric to diff_coef assignent function\n",
    "alfa = -3.5\n",
    "beta = 2.0\n",
    "gamma = 8\n",
    "swc = 2.0 # sample weight coefficient\n",
    "diff_threshold = 50\n",
    "std_threshold_dist = 1.5 # Distance from std to apply sample_weight correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kfar_saba',\n",
       " 'koki_factory',\n",
       " 'new_factory_humid',\n",
       " 'musashi_office',\n",
       " 'shufersal',\n",
       " 'new_factory',\n",
       " 'neve_ilan',\n",
       " 'unknown']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_thresholds = {'neve_ilan': 55,\n",
    "                   'kfar_saba': 55,\n",
    "                   'shufersal': 55,\n",
    "                   'new_factory': 50,\n",
    "                   'new_factory_humid': 50,\n",
    "                   'musashi_office': 40,\n",
    "                   'koki_factory': 40,\n",
    "                   'unknown': 50}\n",
    "default_threshold = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dictionary for the image names of each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_images = defaultdict(list)\n",
    "\n",
    "for site in sites:\n",
    "    site_dir = os.path.join(sites_dir, site)\n",
    "    site_images[site] = os.listdir(site_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kfar_saba', 'koki_factory', 'new_factory_humid', 'musashi_office', 'shufersal', 'new_factory', 'neve_ilan', 'unknown'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_images.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to find the source site of a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_site_and_threshold(im_name):\n",
    "    found_states = [im_name in site_images[site] for site in sites]\n",
    "    \n",
    "    if any(found_states):\n",
    "        site = sites[np.argmax(found_states)]\n",
    "        threshold = site_thresholds[site]\n",
    "    else:\n",
    "        site = 'unknown'\n",
    "        site_images[site].append(im_name)\n",
    "        threshold = default_threshold\n",
    "        \n",
    "    return site, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define curve to assign diff_coef according to diff_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_metric_to_diff_coef(sigma_dist):\n",
    "    \n",
    "    # Correction curve for assigning coefficients\n",
    "    # Based on Sigmoid\n",
    "    # adding alpha, beta and gamma controls, as explained at the\n",
    "    # beginning of this notebook\n",
    "    \n",
    "    return 1/(1 + np.exp(-(sigma_dist*alfa-beta)*gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff[25][:10]: [[18  7 33]\n",
      " [18 13 22]\n",
      " [11 10  8]\n",
      " [11 12  8]\n",
      " [14 13  9]\n",
      " [ 8 10 10]\n",
      " [ 0  0  0]\n",
      " [ 0  0  0]\n",
      " [ 2  1  4]\n",
      " [ 0  0  0]]\n",
      "agg_rgb.shape: (50, 75)\n",
      "diff[25][:10]: [[7 6 0]\n",
      " [3 5 2]\n",
      " [4 7 5]\n",
      " [1 5 3]\n",
      " [2 5 3]\n",
      " [0 3 2]\n",
      " [0 2 4]\n",
      " [1 3 7]\n",
      " [3 5 7]\n",
      " [1 4 6]]\n",
      "agg_rgb.shape: (314, 202)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dict = {'in_name': [],\n",
    "              'out_name': [],\n",
    "              'class_name': [],\n",
    "              'diff_metric': [],\n",
    "              'diff_coef': [],\n",
    "              'sample_weight': []\n",
    "             }\n",
    "diff_metrics = {'no_obstacle': [], 'obstacle': []}\n",
    "class_names = ['no_obstacle', 'obstacle']\n",
    "subset_name = 'train'\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(in_dir, subset_name, class_name)\n",
    "    im_names = os.listdir(class_path)\n",
    "    for i, im_name in tqdm(enumerate(im_names)):\n",
    "        im_path = os.path.join(class_path, im_name)\n",
    "        pair = cv2.imread(im_path)\n",
    "\n",
    "        # Generate diff mask            \n",
    "        w = pair.shape[1]\n",
    "        ref = pair[:, :w//2]\n",
    "        current = pair[:, w//2:(w//2)*2]\n",
    "        diff = cv2.subtract(ref, current)\n",
    "        if i == 0:\n",
    "            print(f'diff.shape: {diff.shape}')\n",
    "            print(f'diff[25][:10]: {diff[25][:10]}')\n",
    "        agg_rgb = np.stack((diff[:, :, 0], diff[:, :, 1], diff[:, :, 2])).max(0)\n",
    "        if i == 0:\n",
    "            print(f'agg_rgb.shape: {agg_rgb.shape}')\n",
    "            print(f'agg_rgb[25][:10]: {agg_rgb[25][:10]}')\n",
    "            break\n",
    "        _, mask = cv2.threshold(agg_rgb, diff_threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Calculate diff_coeff\n",
    "        h = mask.shape[0]\n",
    "        w = mask.shape[1]\n",
    "        area = h * w\n",
    "\n",
    "        # Update train dictionary\n",
    "        train_dict['in_name'].append(im_name)\n",
    "        train_dict['class_name'].append(class_name)\n",
    "        diff_metric = (np.sum(mask)/255)/area\n",
    "        train_dict['diff_metric'].append(diff_metric)    \n",
    "        diff_metrics[class_name].append(diff_metric)\n",
    "            \n",
    "mean = {'no_obstacle': np.mean(diff_metrics['no_obstacle']),\n",
    "        'obstacle': np.mean(diff_metrics['obstacle'])}\n",
    "std = {'no_obstacle': np.std(diff_metrics['no_obstacle']),\n",
    "       'obstacle': np.std(diff_metrics['obstacle']) }\n",
    "\n",
    "for i, diff_metric in enumerate(train_dict['diff_metric']):\n",
    "    class_name = train_dict['class_name'][i]\n",
    "    # Following is to adjust the direction of distance from std and correction accordingly\n",
    "    # For obstacle - a negative sigma means we are lower than threshold and need correction\n",
    "    # For no obstacle a positive sigma means we are higher than threshold and need correction\n",
    "    sigma_dist_sign = 1.0 if class_name == 'obstacle' else -1.0 \n",
    "    diff_threshold = mean[class_name] + sigma_dist_sign * std_threshold_dist * std[class_name]\n",
    "    sigma_dist = sigma_dist_sign * (diff_metric - diff_threshold)/std[class_name]\n",
    "    diff_coef = diff_metric_to_diff_coef(sigma_dist)\n",
    "    sample_weight = 1.0 + swc * diff_coef\n",
    "    train_dict['diff_coef'].append(diff_coef)\n",
    "    train_dict['sample_weight'].append(sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to generate <ref, current, mask> triplet from <ref, current> pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_image(pair, threshold):\n",
    "    w = pair.shape[1]\n",
    "    ref = pair[:, :w//2]\n",
    "    current = pair[:, w//2:(w//2)*2]\n",
    "    diff = cv2.absdiff(current, ref)\n",
    "    agg_rgb = np.stack((diff[:, :, 0], diff[:, :, 1], diff[:, :, 2])).max(0)\n",
    "    _, mask = cv2.threshold(agg_rgb, threshold, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # old morphological operations\n",
    "    copyImg = cv2.erode(mask, np.ones((3, 3), np.uint8), iterations=1)  # reduce noise\n",
    "    copyImg = cv2.dilate(copyImg, np.ones((7, 7), np.uint8), iterations=1)\n",
    "    copyImg = cv2.erode(copyImg, np.ones((5, 5), np.uint8), iterations=1)\n",
    "    copyImg = cv2.dilate(copyImg, np.ones((9, 9), np.uint8), iterations=1)\n",
    "    kernel = np.ones((11, 11), np.uint8)  # kernel for dilation\n",
    "\n",
    "    # increase area to an object\n",
    "    copyImg = cv2.dilate(copyImg, kernel, iterations=2)\n",
    "    copyImg = cv2.dilate(copyImg, np.ones((13, 13), np.uint8), iterations=1)\n",
    "    copyImg = cv2.erode(copyImg, np.ones((11, 11), np.uint8), iterations=1)\n",
    "    copyImg = cv2.erode(copyImg, np.ones((5, 5), np.uint8), iterations=1)\n",
    "\n",
    "    mask = copyImg \n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)  \n",
    "    \n",
    "    return cv2.hconcat([ref, current, mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate triplet images <ref, current, mask>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_names = ['train', 'eval']\n",
    "class_names = ['no_obstacle', 'obstacle']\n",
    "class_extensions = {'no_obstacle': 'noobs', 'obstacle': 'obs'}\n",
    "\n",
    "for subset_name in subset_names:\n",
    "    cur_out_dir = os.path.join(out_dir, subset_name)\n",
    "    Path(cur_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare indices for shuffling the images in dictionary, so onstacle/no_obstacle are mixed\n",
    "# This is necessary for the training/validation from corresponding dataframe to work properly\n",
    "\n",
    "keys = list(train_dict.keys())\n",
    "shuffled_train_dict = {}\n",
    "inds = [i for i in range(len(train_dict['in_name']))]\n",
    "shuffled = inds.copy()\n",
    "random.shuffle(shuffled)\n",
    "for k in keys:\n",
    "    if len(train_dict[k]) > 0:\n",
    "        shuffled_train_dict[k] = [train_dict[k][shuffled[i]] for i in range(len(train_dict['in_name']))]\n",
    "    else:\n",
    "        shuffled_train_dict[k] = []\n",
    "    \n",
    "subset_name = 'train'\n",
    "i = 0\n",
    "for im_name in tqdm(shuffled_train_dict['in_name']):\n",
    "    class_name = shuffled_train_dict['class_name'][i]\n",
    "    class_path = os.path.join(in_dir, subset_name, class_name)\n",
    "    im_path = os.path.join(class_path, im_name)\n",
    "    pair = cv2.imread(im_path)\n",
    "    site, threshold = find_site_and_threshold(im_name)\n",
    "    triplet = triplet_image(pair, threshold)\n",
    "    class_extension = class_extensions[class_name]\n",
    "    sample_weight = shuffled_train_dict['sample_weight'][i]\n",
    "    out_im_name = '.'.join(im_name.split('.')[:-1])\\\n",
    "         + f'_{site}_{class_extension}_{sample_weight:.4f}_.jpg'\n",
    "    shuffled_train_dict['out_name'].append(out_im_name)\n",
    "    cur_out_dir = os.path.join(out_dir, subset_name)\n",
    "    out_path = os.path.join(cur_out_dir, out_im_name)\n",
    "    cv2.imwrite(out_path, triplet)\n",
    "    i += 1\n",
    "    \n",
    "subset_name = 'eval'\n",
    "for class_name in class_names:\n",
    "    class_path = os.path.join(in_dir, subset_name, class_name)\n",
    "    im_names = os.listdir(class_path)\n",
    "    cur_out_dir = os.path.join(out_dir, subset_name, class_name)\n",
    "    Path(cur_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "    for im_name in tqdm(im_names):\n",
    "        im_path = os.path.join(class_path, im_name)\n",
    "        pair = cv2.imread(im_path)\n",
    "        site, threshold = find_site_and_threshold(im_name)\n",
    "        triplet = triplet_image(pair, threshold)\n",
    "        out_im_name = '.'.join(im_name.split('.')[:-1]) + f'_{site}_.jpg'\n",
    "        out_path = os.path.join(cur_out_dir, out_im_name)\n",
    "        cv2.imwrite(out_path, triplet)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe from sample_weights Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(shuffled_train_dict[k]) for k in shuffled_train_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_dict(shuffled_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the Dataframe in a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = out_dir.split('/')[-1] + '.csv'\n",
    "csv_path = os.path.join(out_dir, csv_name)\n",
    "train_df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean['obstacle'], std['obstacle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean['no_obstacle'], std['no_obstacle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = 0\n",
    "for site in site_images:\n",
    "    tot += len(site_images[site])\n",
    "    print(f'{site}: {len(site_images[site])}')\n",
    "print(f'========== total: {tot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
