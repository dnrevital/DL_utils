{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-05T09:05:42.307352Z",
     "iopub.status.busy": "2021-07-05T09:05:42.306996Z",
     "iopub.status.idle": "2021-07-05T09:05:42.315414Z",
     "shell.execute_reply": "2021-07-05T09:05:42.314278Z",
     "shell.execute_reply.started": "2021-07-05T09:05:42.307324Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.models import Model\n",
    "import boto3\n",
    "from time import time\n",
    "import os\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from abc import ABC, abstractmethod\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes to handle dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler(ABC):\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 obstacle_dataset,\n",
    "                 no_obstacle_dataset,\n",
    "                 img_width=200,\n",
    "                 img_height=200,\n",
    "                 batch_size=32,\n",
    "                 rand_images=True,\n",
    "                 num_rand_images=10,\n",
    "                 thresh_csv_fname=None,\n",
    "                 metrics_csv_fname=None):\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self.obstacle_dataset = obstacle_dataset\n",
    "        self.no_obstacle_dataset = no_obstacle_dataset\n",
    "        self.dataset = '/'.join(self.obstacle_dataset.split('/')[:-1])\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.obstacle_images = []\n",
    "        self.no_obstacle_images = []\n",
    "        self._update_image_lists = False\n",
    "        self.rand_images = rand_images\n",
    "        \n",
    "        if rand_images:\n",
    "            self.batch_size = min(batch_size, num_rand_images*2)\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            \n",
    "        if rand_images:\n",
    "            self.num_rand_images = num_rand_images\n",
    "            self.num_obstacles = num_rand_images\n",
    "            self.num_no_obstacles = num_rand_images\n",
    "            self.num_images = num_rand_images*2\n",
    "        else:\n",
    "            # Will be determined while reading all images from dataset\n",
    "            self.num_obstacles = 0\n",
    "            self.num_no_obstacles = 0\n",
    "            self.num_images = 0\n",
    "            \n",
    "        if rand_images:\n",
    "            self.obstacle_image_names = self._get_random_dataset_image_names(self.obstacle_dataset)\n",
    "            self.no_obstacle_image_names = self._get_random_dataset_image_names(self.no_obstacle_dataset)\n",
    "        else:\n",
    "            (self.obstacle_image_names,\n",
    "            self.num_obstacles) = self._get_all_dataset_image_names(self.obstacle_dataset)\n",
    "            (self.no_obstacle_image_names,\n",
    "            self.num_no_obstacles) = self._get_all_dataset_image_names(self.no_obstacle_dataset)\n",
    "            \n",
    "        self.model_score = ModelScore()\n",
    "            \n",
    "        if thresh_csv_fname:\n",
    "            thresh_csv_path = os.path.join(model_path, thresh_csv_fname)\n",
    "            self.f_thresh_csv = open(thresh_csv_path, 'w')\n",
    "            self.thresh_csv_writer = csv.writer(self.f_thresh_csv)\n",
    "            \n",
    "            self.thresh_csv_header = ['model',\n",
    "                                      'dataset',\n",
    "                                      'Max Lost Ratio',\n",
    "                                      'Lower Threshold',\n",
    "                                      'Upper Threshold',\n",
    "                                      'Total Predictions',\n",
    "                                      'TP',\n",
    "                                      '% TP',\n",
    "                                      'TN',\n",
    "                                      '% TN',\n",
    "                                      'FP',\n",
    "                                      '% FP',\n",
    "                                      'FN',\n",
    "                                      '% FN',\n",
    "                                      'Not Decided',\n",
    "                                      '% Not Decided',\n",
    "                                      'True Not Decided',\n",
    "                                      '% True Not Decided']\n",
    "            \n",
    "            self.thresh_csv_writer.writerow(self.thresh_csv_header)\n",
    "            \n",
    "        if metrics_csv_fname:\n",
    "            metrics_csv_path = os.path.join(model_path, metrics_csv_fname)\n",
    "            self.f_metrics_csv = open(metrics_csv_path, 'w')\n",
    "            self.metrics_csv_writer = csv.writer(self.f_metrics_csv)\n",
    "\n",
    "            metrics_prefixes = ['test', 'validation', '']\n",
    "            metrics_separators = [' ', ' ', '']\n",
    "            metrics_functions = ['auc', 'recall', 'specifity']\n",
    "            \n",
    "            self.metrics_csv_header = ['Trial Name']\n",
    "            self.metrics_csv_header.append('Model Score')\n",
    "            self.metrics_csv_header.append('Lost True Values %')\n",
    "            self.metrics_csv_header.append('Lower Threshold')\n",
    "            self.metrics_csv_header.append('Upper Threshold')\n",
    "            self.metrics_csv_header.append('% FP')\n",
    "            self.metrics_csv_header.append('% FN')\n",
    "            \n",
    "            for i, metrics_prefix in enumerate(metrics_prefixes):\n",
    "                for metrics_function in metrics_functions:\n",
    "                    metric_name = metrics_separators[i].join([metrics_prefix, metrics_function])\n",
    "                    self.metrics_csv_header.append('max ' + metric_name)\n",
    "            \n",
    "            self.metrics_csv_writer.writerow(self.metrics_csv_header)\n",
    "\n",
    "    def write_csv(self, **args):        \n",
    "        row = []\n",
    "        keys = args.keys()\n",
    "        \n",
    "        if 'model' in keys:\n",
    "            row.append(args['model'])\n",
    "            \n",
    "        row.append(self.dataset)\n",
    "        \n",
    "        for col in self.thresh_csv_header[2:]:\n",
    "            if col in keys:\n",
    "                row.append(args[col])\n",
    "            else:\n",
    "                row.append(None)\n",
    "        \n",
    "        self.thresh_csv_writer.writerow(row)\n",
    "                \n",
    "    @abstractmethod\n",
    "    def _get_random_dataset_image_names(self, dataset):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_all_dataset_image_names(self, dataset):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_image(self, imname):\n",
    "        pass\n",
    "    \n",
    "    def _preprocess_image(self, im):\n",
    "        w = im.shape[1]\n",
    "        im1 = im[:,:w//2]\n",
    "        im2 = im[:,w//2:]\n",
    "        sub = cv2.subtract(im1, im2)\n",
    "        sub = sub.reshape(sub.shape[0], sub.shape[1], 1)\n",
    "        arr = keras.preprocessing.image.smart_resize(sub,\n",
    "                                                     (self.img_height, self.img_width),\n",
    "                                                     interpolation='bilinear')\n",
    "        arr /= 255.0\n",
    "        return arr\n",
    "    \n",
    "    def image_generator(self):\n",
    "        inputs = []\n",
    "        outputs = [0]*self.num_no_obstacles + [1]*self.num_obstacles\n",
    "\n",
    "        for image_name in tqdm(self.no_obstacle_image_names):\n",
    "            im = self._get_image(image_name)\n",
    "            if self._update_image_lists:\n",
    "                self.no_obstacle_images.append(im)\n",
    "            im = self._preprocess_image(im)\n",
    "            inputs.append(im)\n",
    "\n",
    "        for image_name in tqdm(self.obstacle_image_names):\n",
    "            im = self._get_image(image_name)\n",
    "            if self._update_image_lists:\n",
    "                self.obstacle_images.append(im)\n",
    "            im = self._preprocess_image(im)\n",
    "            inputs.append(im)\n",
    "            \n",
    "        self._update_image_lists = True\n",
    "\n",
    "        for i in range(0, self.num_images, self.batch_size):\n",
    "            x = np.array(inputs[i:i+self.batch_size])\n",
    "            y = np.array(outputs[i:i+self.batch_size])\n",
    "            yield(x, y)\n",
    "            \n",
    "    @abstractmethod\n",
    "    def get_metrics(self, model):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_predictions(self, model, color_mode='rgb'):\n",
    "        pass\n",
    "    \n",
    "    def print_model_metrics(self, model, color_mode):\n",
    "        metrics = self.get_metrics(model, color_mode)\n",
    "\n",
    "        for name, value in zip(model.metrics_names, metrics):\n",
    "            print(name, ': ', value)\n",
    "            \n",
    "    # Compute and plot confusion-matrix (TP, FP, TN, FN)\n",
    "    def calc_cm(self,\n",
    "                model_path,\n",
    "                labels,\n",
    "                predictions,\n",
    "                p=0.5,\n",
    "                max_lost_ratio=0,\n",
    "                not_decided=0,\n",
    "                trues=0,\n",
    "                true_not_decided=0,\n",
    "                lower_threshold=None,\n",
    "                upper_threshold=None,\n",
    "                print_params=True,\n",
    "                plot=True,\n",
    "                print_to_csv=False):\n",
    "\n",
    "        cm = confusion_matrix(labels, predictions > p)        \n",
    "\n",
    "        if plot:\n",
    "            plt.figure(figsize=(5,5))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "            if max_lost_ratio:\n",
    "                plt.title('Confusion matrix @ max lost ratio {:.2f}'.format(max_lost_ratio))\n",
    "            else:\n",
    "                plt.title('Confusion matrix @ threshold {:.2f}'.format(p))\n",
    "            plt.ylabel('Actual label')\n",
    "            plt.xlabel('Predicted label')\n",
    "\n",
    "        predicts = sum(sum(x) for x in cm)\n",
    "        tot_predicts = predicts + not_decided\n",
    "        \n",
    "        TN = cm[0][0]\n",
    "        FP = cm[0][1]\n",
    "        FN = cm[1][0]\n",
    "        TP = cm[1][1]\n",
    "        \n",
    "        TN_percent = (TN/tot_predicts)*100\n",
    "        FP_percent = (FP/tot_predicts)*100\n",
    "        FN_percent = (FN/tot_predicts)*100\n",
    "        TP_percent = (TP/tot_predicts)*100\n",
    "        not_decided_percent = (not_decided/tot_predicts)*100\n",
    "        true_not_decided_percent = (true_not_decided/trues)*100\n",
    "\n",
    "        if print_params:\n",
    "            print()\n",
    "            if max_lost_ratio:\n",
    "                print(f'Max Lost Ratio: {max_lost_ratio}')\n",
    "            print(f'No Obstacles Detected (True Negatives): {TN} ({TN_percent:.2f}%)')\n",
    "            print(f'No Obstacles Incorrectly Detected (False Positives): {FP} ({FP_percent:.2f}%)')\n",
    "            print(f'Obstacles Missed (False Negatives): {FN} ({FN_percent:.2f}%)')\n",
    "            print(f'Obstacles Detected (True Positives): {TP} ({TP_percent:.2f}%)')\n",
    "            if not_decided:\n",
    "                print(f'Not Decided: {not_decided} ({not_decided_percent:.2f}%)')\n",
    "            if true_not_decided:\n",
    "                print(f'True Not Decided: {true_not_decided} ({true_not_decided_percent:.2f}%)')\n",
    "            print(f'Total Obstacles: {tot_predicts}')\n",
    "        \n",
    "        self.model_score.add_model_option(true_not_decided,\n",
    "                                          true_not_decided_percent/100.0,\n",
    "                                          FP,\n",
    "                                          FP_percent/100.0,\n",
    "                                          FN,\n",
    "                                          FN_percent/100.0,\n",
    "                                          lower_threshold,\n",
    "                                          upper_threshold,\n",
    "                                          predictions=predictions)\n",
    "        \n",
    "        if print_to_csv:\n",
    "            # Prepare arguments for writing to csv\n",
    "            args = {'model': model_path,\n",
    "                    'Max Lost Ratio': max_lost_ratio,\n",
    "                    'Lower Threshold': lower_threshold,\n",
    "                    'Upper Threshold': upper_threshold,\n",
    "                    'Total Predictions': tot_predicts,\n",
    "                    'TP': TP,\n",
    "                    '% TP': TP_percent,\n",
    "                    'TN': TN,\n",
    "                    '% TN': TN_percent,\n",
    "                    'FP': FP,\n",
    "                    '% FP': FP_percent,\n",
    "                    'FN': FN,\n",
    "                    '% FN': FN_percent,\n",
    "                    'Not Decided': not_decided,\n",
    "                    '% Not Decided': not_decided_percent,\n",
    "                    'True Not Decided': true_not_decided,\n",
    "                    '% True Not Decided': true_not_decided_percent}\n",
    "            self.write_csv(**args)\n",
    "        \n",
    "        \n",
    "    # Same as above, with consideration of lower and upper threshold \n",
    "    # For values between those thresholds \"not decided\" is returned, \n",
    "    # so it lowers the no. of FP and FN\n",
    "    def calc_cm_considering_thresholds(self,\n",
    "                                       model_path,\n",
    "                                       labels,\n",
    "                                       predictions,\n",
    "                                       lower_threshold,\n",
    "                                       upper_threshold,\n",
    "                                       max_lost_ratio=0,\n",
    "                                       threshold=0.5,\n",
    "                                       print_params=True,\n",
    "                                       plot=True,\n",
    "                                       print_to_csv=False):\n",
    "        no_obs_preds = predictions[:self.num_no_obstacles]\n",
    "        no_obs_margin_preds = [p for p in no_obs_preds if p < lower_threshold or p > upper_threshold]\n",
    "        no_obs_margin_labels = [0]*len(no_obs_margin_preds)\n",
    "        true_no_obs_preds = [p for p in no_obs_preds if p <= 0.5]\n",
    "        true_no_obs_not_decided_preds = [p for p in no_obs_preds if lower_threshold < p <= 0.5]\n",
    "        obs_preds = predictions[self.num_no_obstacles:]\n",
    "        obs_margin_preds = [p for p in obs_preds if p < lower_threshold or p > upper_threshold]\n",
    "        obs_margin_labels = [1]*len(obs_margin_preds)\n",
    "        true_obs_preds = [p for p in obs_preds if p >= 0.5]\n",
    "        true_obs_not_decided_preds = [p for p in obs_preds if upper_threshold > p >= 0.5]\n",
    "        margin_preds = no_obs_margin_preds + obs_margin_preds\n",
    "        margin_preds = np.array(margin_preds, dtype=float)\n",
    "        true_preds = true_no_obs_preds + true_obs_preds\n",
    "        true_not_decided_preds = true_no_obs_not_decided_preds + true_obs_not_decided_preds\n",
    "        margin_labels = no_obs_margin_labels + obs_margin_labels\n",
    "        margin_labels = np.array(margin_labels, dtype=float)\n",
    "        not_decided = len(predictions) - len(margin_preds)\n",
    "        trues = len(true_preds)\n",
    "        true_not_decided = len(true_not_decided_preds)\n",
    "\n",
    "        self.calc_cm(model_path,\n",
    "                     margin_labels,\n",
    "                     margin_preds,\n",
    "                     max_lost_ratio=max_lost_ratio,\n",
    "                     not_decided=not_decided,\n",
    "                     trues=trues,\n",
    "                     true_not_decided=true_not_decided,\n",
    "                     lower_threshold=lower_threshold,\n",
    "                     upper_threshold=upper_threshold,\n",
    "                     print_params=print_params,\n",
    "                     plot=plot,\n",
    "                     print_to_csv=print_to_csv)\n",
    "    \n",
    "    def display_false_negatives(self, predictions, filenames, lower_threshold):\n",
    "        image_names = [s.split('/')[-1] for s in self.obstacle_image_names]\n",
    "        false_negatives = [fname for i, fname in enumerate(filenames)\\\n",
    "                           if fname.split('/')[-1] in image_names\\\n",
    "                           and predictions[i] < lower_threshold]\n",
    "        preds = [p for i, p in enumerate(predictions)\\\n",
    "                 if filenames[i].split('/')[-1] in image_names\\\n",
    "                 and p < lower_threshold]\n",
    "        \n",
    "        if 500 > len(false_negatives) > 1:\n",
    "            num_images = len(false_negatives)\n",
    "            _, axarr = plt.subplots(num_images, 1, figsize=(num_images, num_images))\n",
    "            \n",
    "            for i, fname in enumerate(false_negatives):\n",
    "                imname = fname.split('/')[-1]\n",
    "                impath = os.path.join(self.obstacle_dataset, imname)\n",
    "                im = cv2.imread(impath)\n",
    "                im1 = im[:, :, 0]\n",
    "                im2 = im[:, :, 1]\n",
    "                im12 = cv2.hconcat([im1, im2])\n",
    "                axarr[i].imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "                axarr[i].set_title(str(preds[i]) + ' ' + imname + (' '*30), fontdict={'fontsize': 10})\n",
    "            plt.show()\n",
    "        elif false_negatives:\n",
    "            fname = false_positives[0]\n",
    "            imname = fname.split('/')[-1]\n",
    "            impath = os.path.join(self.obstacle_dataset, imname)\n",
    "            im = cv2.imread(impath)\n",
    "            im1 = im[:, :, 0]\n",
    "            im2 = im[:, :, 1]\n",
    "            im12 = cv2.hconcat([im1, im2])\n",
    "            plt.imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "            plt.title(str(preds[0])+ '\\n' + imname)\n",
    "            plt.show()   \n",
    "            \n",
    "    def display_false_positives(self, predictions, filenames, upper_threshold):\n",
    "        image_names = [s.split('/')[-1] for s in self.no_obstacle_image_names]\n",
    "        false_positives = [fname for i, fname in enumerate(filenames)\\\n",
    "                           if fname.split('/')[-1] in image_names\\\n",
    "                           and predictions[i] > upper_threshold]\n",
    "        preds = [p for i, p in enumerate(predictions)\\\n",
    "                 if filenames[i].split('/')[-1] in image_names\\\n",
    "                 and p > upper_threshold]\n",
    "\n",
    "        if 500 > len(false_positives) > 1:\n",
    "            num_images = len(false_positives)\n",
    "            _, axarr = plt.subplots(num_images, 1, figsize=(num_images, num_images))\n",
    "\n",
    "            for i, fname in enumerate(false_positives):\n",
    "                imname = fname.split('/')[-1]\n",
    "                impath = os.path.join(self.no_obstacle_dataset, imname)\n",
    "                im = cv2.imread(impath)\n",
    "                im1 = im[:, :, 0]\n",
    "                im2 = im[:, :, 1]\n",
    "                im12 = cv2.hconcat([im1, im2])\n",
    "                print(f'FP prediction: {preds[i]}, imname: {imname}')\n",
    "                axarr[i].imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "                axarr[i].set_title(str(preds[i]) + ' ' + imname + (' '*30))\n",
    "            plt.show()\n",
    "        elif false_positives:\n",
    "            fname = false_positives[0]\n",
    "            imname = fname.split('/')[-1]\n",
    "            impath = os.path.join(self.no_obstacle_dataset, imname)\n",
    "            im = cv2.imread(impath)\n",
    "            im1 = im[:, :, 0]\n",
    "            im2 = im[:, :, 1]\n",
    "            im12 = cv2.hconcat([im1, im2])\n",
    "            plt.imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "            plt.title(str(preds[0]) + ' ' + imname)\n",
    "            plt.show()\n",
    "\n",
    "    def display_not_decided(self, predcitions, filenames, lower_threshold, upper_threshold):\n",
    "        obstacle_image_names = [s.split('/')[-1] for s in self.obstacle_image_names]\n",
    "        no_obstacle_image_names = [s.split('/')[-1] for s in self.no_obstacle_image_names]\n",
    "        obs_title = ''\n",
    "        \n",
    "        not_decided = [fname for i, fname in enumerate(filenames)\\\n",
    "                       if ((lower_threshold < predictions[i] <= 0.5)\\\n",
    "                           and (fname.split('/')[-1] in no_obstacle_image_names))\\\n",
    "                           or ((0.5 <= predictions[i] < upper_threshold)\\\n",
    "                           and (fname.split('/')[-1] in obstacle_image_names))]\n",
    "        \n",
    "        preds = [p for i, p in enumerate(predictions)\\\n",
    "                       if ((lower_threshold < p <= 0.5)\\\n",
    "                           and (filenames[i].split('/')[-1] in no_obstacle_image_names))\\\n",
    "                           or ((0.5 <= p < upper_threshold)\\\n",
    "                           and (filenames[i].split('/')[-1] in obstacle_image_names))]\n",
    "\n",
    "        if 500 > len(not_decided) > 1:\n",
    "            num_images = len(not_decided)\n",
    "            _, axarr = plt.subplots(num_images, 1, figsize=(1.5*num_images, 1.5*num_images))\n",
    "\n",
    "            for i, fname in enumerate(not_decided):\n",
    "                imname = fname.split('/')[-1]\n",
    "                if imname in obstacle_image_names:\n",
    "                    impath = os.path.join(self.obstacle_dataset, imname)\n",
    "                    obs_title = 'obs'\n",
    "                else:\n",
    "                    impath = os.path.join(self.no_obstacle_dataset, imname)\n",
    "                    obs_title = 'no_obs'\n",
    "                im = cv2.imread(impath)\n",
    "                im1 = im[:, :, 0]\n",
    "                im2 = im[:, :, 1]\n",
    "                im12 = cv2.hconcat([im1, im2])\n",
    "                axarr[i].imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "                axarr[i].set_title(' '.join([imname, obs_title, str(preds[i]), (' '*30)]))\n",
    "            plt.show()\n",
    "        elif not_decided:\n",
    "            imname = not_decided[0].split('/')[-1]\n",
    "            if imname in obstacle_image_names:\n",
    "                impath = os.path.join(self.obstacle_dataset, imname)\n",
    "                obs_title = 'obs'\n",
    "            else:\n",
    "                impath = os.path.join(self.no_obstacle_dataset, imname)\n",
    "                obs_title = 'no_obs'\n",
    "            im = cv2.imread(impath)\n",
    "            im1 = im[:, :, 0]\n",
    "            im2 = im[:, :, 1]\n",
    "            im12 = cv2.hconcat([im1, im2])\n",
    "            print(f'ND prediction: {preds[0]}')\n",
    "            plt.imshow(im12, cmap='gray', vmin=0, vmax=255)\n",
    "            plt.title(' '.join([imname, obs_title, str(preds[0])]))\n",
    "            plt.show()\n",
    "\n",
    "    # An algorithm for optimal threshold taken from \n",
    "    # https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "    # left here on case we consider re-using it.\n",
    "    # Currently we are using the 2nd algorithm below\n",
    "    def optimal_threshold_by_gmeans(self, num_no_obstacles, num_obstacles, predictions):\n",
    "        true_values = [0]*num_no_obstacles + [1]*num_obstacles \n",
    "        fpr, tpr, thresholds = roc_curve(true_values, predictions)\n",
    "        gmeans = sqrt(tpr * (1-fpr))\n",
    "        ix = argmax(gmeans)\n",
    "\n",
    "        return thresholds[ix]    \n",
    " \n",
    "    # 2nd algorithm for finding optimal threshold, own developed\n",
    "    # Currently used\n",
    "    # Params: \n",
    "    #    -- predictions - a list of predicted values for no_obstacle / obstacle\n",
    "    #    -- max_lost_ratio - a float [0.0-1.0], indicating the target maximum ratio (%/100)\n",
    "    #    of (\"not decided\" / all) true values, for TP and TN - this is to avoid putting extreme \n",
    "    #    low/high thresholds, which will lower FN/FP, but will leave too few True values returned.\n",
    "    #    This ratio (\"not_decided\" / all) is called here \"lost ratio\"\n",
    "    # Steps:\n",
    "    # 1. Set as (min_thresold / max_thresold) ==> (min(FN) / max(FP))\n",
    "    #       Using this will perform with 0 FN/FP, \n",
    "    #       as there are neither FN below lower_thresold, not FP above upper_threshold,\n",
    "    #       but --potentially-- with too many \"not decided\"\n",
    "    # 2. If the lost ratio is too high - start lowering it to the target, using binary search\n",
    "    #       to find the point where the ratio does not exceed the maximum lost-ration,\n",
    "    #       while keeping the lower/upper thresholds as close as possible to their \n",
    "    #       original values. \n",
    "    def find_thresholds(self, predictions, max_lost_ratio=0.1):\n",
    "        no_obs_preds = predictions[:handler.num_no_obstacles]\n",
    "        obs_preds = predictions[handler.num_no_obstacles:]\n",
    "        false_positives = np.array([p for p in no_obs_preds if p >= 0.5])\n",
    "        false_negatives = np.array([p for p in obs_preds if p < 0.5])\n",
    "        true_positives = np.array([p for p in obs_preds if p >= 0.5])\n",
    "        true_negatives = np.array([p for p in no_obs_preds if p < 0.5])\n",
    "        false_positives.sort(axis=0) \n",
    "        false_negatives.sort(axis=0)\n",
    "        true_positives.sort(axis=0)\n",
    "        true_negatives.sort(axis=0)\n",
    "\n",
    "        true_positives_above = np.array([p for p in true_positives if p > max(false_positives)])\n",
    "        true_negatives_below = np.array([p for p in true_negatives if p < min(false_negatives)])\n",
    "        lost_true_positives = np.array([p for p in true_positives if p <= max(false_positives)])\n",
    "        true_positives_lost_ratio = len(lost_true_positives) / len(true_positives)\n",
    "        lost_true_negatives = np.array([p for p in true_negatives if p >= min(false_negatives)])\n",
    "        true_negatives_lost_ratio = len(lost_true_negatives) / len(true_negatives)\n",
    "\n",
    "        lower_threshold = min(false_negatives)\n",
    "        lower_threshold_ind = np.where(true_negatives > lower_threshold)[0][0]\n",
    "        num_true_negatives = len(true_negatives)\n",
    "        gap = num_true_negatives - lower_threshold_ind\n",
    "        \n",
    "        # Binary search to find the point where no. of \"not decided\" satisfies the target\n",
    "\n",
    "        if true_negatives_lost_ratio > max_lost_ratio:\n",
    "            while gap:\n",
    "                lost_ratio_diff = true_negatives_lost_ratio - max_lost_ratio\n",
    "                if abs(lost_ratio_diff) < 0.01:\n",
    "                    break\n",
    "                gap //= 2\n",
    "                if lost_ratio_diff < 0.0:\n",
    "                    lower_threshold_ind -= gap\n",
    "                else:\n",
    "                    lower_threshold_ind += gap\n",
    "                lost_true_negatives = true_negatives[lower_threshold_ind:]\n",
    "                true_negatives_lost_ratio = len(lost_true_negatives) / num_true_negatives\n",
    "\n",
    "            lower_threshold = true_negatives[lower_threshold_ind] \n",
    "            \n",
    "        upper_threshold = max(false_positives)\n",
    "        upper_threshold_ind = np.where(true_positives > upper_threshold)[0][0]\n",
    "        num_true_positives = len(true_positives)\n",
    "        gap = upper_threshold_ind\n",
    "\n",
    "        # Binary search to find the point where no. of \"not decided\" satisfies the target\n",
    "        \n",
    "        if true_positives_lost_ratio > max_lost_ratio:\n",
    "            while gap:\n",
    "                lost_ratio_diff = true_positives_lost_ratio - max_lost_ratio\n",
    "                if abs(lost_ratio_diff) < 0.01:\n",
    "                    break\n",
    "                gap //= 2\n",
    "                if lost_ratio_diff < 0.0:\n",
    "                    upper_threshold_ind += gap\n",
    "                else:\n",
    "                    upper_threshold_ind -= gap\n",
    "                lost_true_positives = true_positives[:upper_threshold_ind]\n",
    "                true_positives_lost_ratio = len(lost_true_positives) / num_true_positives\n",
    "\n",
    "            upper_threshold = true_positives[upper_threshold_ind]\n",
    "            \n",
    "        return lower_threshold[0], upper_threshold[0]\n",
    "    \n",
    "    def write_metrics_to_csv(self, trial_component_display_name, print_params=True):\n",
    "        client = boto3.client('sagemaker', region_name='eu-west-1')\n",
    "        trials = client.list_trial_components()['TrialComponentSummaries']\n",
    "        trial = [t for t in trials if t['DisplayName'] == trial_component_display_name]\n",
    "        trial_description = None\n",
    "        \n",
    "        if trial:\n",
    "            trial_component_name = trial[0]['TrialComponentName']\n",
    "            trial_description = client.describe_trial_component(TrialComponentName=trial_component_name)\n",
    "\n",
    "        metrics_values = [trial_component_display_name]        \n",
    "\n",
    "        best_option = self.model_score.scored_models[self.model_score.best_option]\n",
    "        metrics_values.append(best_option['score'])\n",
    "        metrics_values.append(best_option['lost_true_values_percentage'])\n",
    "        metrics_values.append(best_option['lower_threshold'])\n",
    "        metrics_values.append(best_option['upper_threshold'])\n",
    "        metrics_values.append(best_option['false_positives_percentage'])\n",
    "        metrics_values.append(best_option['false_negatives_percentage'])\n",
    "        \n",
    "        if print_params:\n",
    "            print(f'best option: {best_option[\"lost_true_values_percentage\"]:.2f}')\n",
    "            print(f'score: {best_option[\"score\"]}')\n",
    "            print(f'lower_threshold: {best_option[\"lower_threshold\"]:.2f}')\n",
    "            print(f'upper_threshold: {best_option[\"upper_threshold\"]:.2f}')\n",
    "            print(f'% FP: {best_option[\"false_positives_percentage\"]:.2f}')\n",
    "            print(f'%FN: {best_option[\"false_negatives_percentage\"]:.2f}')\n",
    "\n",
    "        if trial_description:\n",
    "            metrics = trial_description['Metrics']\n",
    "\n",
    "            metrics_prefixes = ['test', 'validation', '']\n",
    "            metrics_separators = [' ', ' ', '']\n",
    "            metrics_functions = ['auc', 'recall', 'specifity']        \n",
    "\n",
    "            for i, metrics_prefix in enumerate(metrics_prefixes):\n",
    "                for metrics_function in metrics_functions:\n",
    "                    metric_name = metrics_separators[i].join([metrics_prefix, metrics_function])\n",
    "                    metric_value = [m for m in metrics if m['MetricName'] == metric_name]\n",
    "                    metrics_values.append(metric_value[0]['Max'])\n",
    "                                \n",
    "        self.metrics_csv_writer.writerow(metrics_values) \n",
    "        \n",
    "    # Method to run when class is deleted\n",
    "    def __del__(self):\n",
    "        self.f_thresh_csv.close()\n",
    "        self.f_metrics_csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CloudDatasetHandler(DatasetHandler):\n",
    "    \n",
    "    def _get_random_dataset_image_names(self, dataset):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = 'obstacles-classification'\n",
    "        image_names = []\n",
    "\n",
    "        paginator = client.get_paginator('list_objects')\n",
    "        page_iterator = paginator.paginate(Bucket=bucket, Prefix=dataset)\n",
    "\n",
    "        for page in page_iterator:\n",
    "            for image_name in page['Contents']:\n",
    "                if image_name['Key'].split('.')[-1] == 'jpg':\n",
    "                    image_names.append(image_name['Key'])\n",
    "\n",
    "        image_names = np.array(image_names)\n",
    "        np.random.shuffle(image_names)\n",
    "        image_names = image_names[:self.num_rand_images]\n",
    "\n",
    "        return list(image_names)\n",
    "\n",
    "    def _get_all_dataset_image_names(self, dataset):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = 'obstacles-classification'\n",
    "        image_names = []\n",
    "        num_images = 0\n",
    "\n",
    "        paginator = client.get_paginator('list_objects')\n",
    "        page_iterator = paginator.paginate(Bucket=bucket, Prefix=dataset)\n",
    "\n",
    "        for page in page_iterator:\n",
    "            for image_name in page['Contents']:\n",
    "                if image_name['Key'].split('.')[-1] == 'jpg':\n",
    "                    image_names.append(image_name['Key'])\n",
    "                    num_images += 1\n",
    "\n",
    "        return image_names, num_images\n",
    "            \n",
    "    def _get_image(self, imname):\n",
    "        client = boto3.client('s3')\n",
    "        bucket = 'obstacles-classification'\n",
    "        key = imname\n",
    "        outfile = io.BytesIO()\n",
    "        client.download_fileobj(bucket, key, outfile)\n",
    "        outfile.seek(0)\n",
    "        im = plt.imread(outfile, format='jpg')\n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def get_metrics(self, model):\n",
    "        return model.evaluate_generator(self.image_generator(), verbose=1)\n",
    "    \n",
    "    def get_predictions(self,model, color_mode='rgb'):\n",
    "        return model.predict_generator(self.image_generator(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalDatasetHandler(DatasetHandler):\n",
    "        \n",
    "    def _get_random_dataset_image_names(self, dataset):\n",
    "        image_names = os.listdir(dataset)\n",
    "        image_paths = [os.path.join(dataset, image_name) for image_name in image_names]\n",
    "        image_paths = np.array(image_paths)\n",
    "        np.random.shuffle(image_paths)\n",
    "        image_paths = image_paths[:self.num_rand_images]\n",
    "\n",
    "        return list(image_paths)\n",
    "\n",
    "    def _get_all_dataset_image_names(self, dataset):\n",
    "        image_names = os.listdir(dataset)\n",
    "        image_paths = [os.path.join(dataset, image_name) for image_name in image_names]\n",
    "        \n",
    "        return image_paths, len(image_paths)\n",
    "            \n",
    "    def _get_image(self, imname):\n",
    "        imrgb = cv2.imread(imname)\n",
    "        im = cv2.cvtColor(imrgb, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def get_metrics(self, model, color_mode='rgb'):\n",
    "        if self.rand_images:\n",
    "            return model.evaluate_generator(self.image_generator(), verbose=1)\n",
    "        else:\n",
    "            datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "            val_generator = datagen.flow_from_directory(\n",
    "                self.dataset,\n",
    "                target_size=(self.img_width, self.img_height),\n",
    "                color_mode=color_mode,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                class_mode='binary'\n",
    "            )\n",
    "            \n",
    "            return model.evaluate_generator(val_generator, verbose=1)\n",
    "        \n",
    "    def get_predictions(self, model, color_mode='rgb'):\n",
    "        if self.rand_images:\n",
    "            return model.predict_generator(self.image_generator(), verbose=1)\n",
    "        else:\n",
    "            datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "            val_generator = datagen.flow_from_directory(\n",
    "                self.dataset,\n",
    "                target_size=(self.img_width, self.img_height),\n",
    "                color_mode=color_mode,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                class_mode='binary'\n",
    "            )\n",
    "\n",
    "            ret = model.predict_generator(val_generator, verbose=1)\n",
    "            filenames = val_generator.filenames\n",
    "            \n",
    "            return ret, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to calculate model's score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelScore():\n",
    "    def __init__(self, alpha=0.7, beta=1.2, gamma=30):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.max_score = 0.0\n",
    "        self.scored_model_ind = 0\n",
    "        self.scored_models = []\n",
    "        \n",
    "    '''\n",
    "    Cliff shape function\n",
    "    \n",
    "    Based on the Sigmoid function:\n",
    "        1/(1 + np.exp(-x))\n",
    "\n",
    "    with:\n",
    "    1. Changing (x) to (1-x), so we flip the curve to be high near 0 and decline sharply at some point, till reaching zero\n",
    "    2. Adding alpha, beta and gamma modifiers to enable controlling the curve's attributes:\n",
    "        * alpha controls the point where the graph starts to decline sharply (= the % of lost true values which is berable and should get relatively high score for this aspect)\n",
    "        * beta controls the width of the sharply declining portion of the curve\n",
    "        * gamma controls the smoothness of the cliff-shape part of the graph. A high gamma will make the curvie less somooth, i.e. more 'cliffy'. \n",
    "\n",
    "    So our Sigmoid-modofied function is:\n",
    "\n",
    "        1/(1 + np.exp(-(1-(x+alpha)*beta)*gamma)) \n",
    "    '''\n",
    "    def cliff(self, x):\n",
    "        return 1/(1 + np.exp(-(1-(x+self.alpha)*self.beta)*self.gamma)) \n",
    "    \n",
    "    # A method to insert into overall model's scores an option, regarding \n",
    "    # the lost_true_value_percentage. \n",
    "    # The goal is that the class remembers the score attached to this option,\n",
    "    # then can compare all scores and find the best one\n",
    "    def add_model_option(self,\n",
    "                         lost_true_values,\n",
    "                         lost_true_values_percentage,\n",
    "                         false_positives,\n",
    "                         false_positives_percentage,\n",
    "                         false_negatives,\n",
    "                         false_negatives_percentage,\n",
    "                         lower_threshold,\n",
    "                         upper_threshold,\n",
    "                         predictions):\n",
    "        \n",
    "        s1 = self.cliff(lost_true_values_percentage)\n",
    "        s2 = 1.0 - (false_positives_percentage + false_negatives_percentage)\n",
    "        score = (s1 + s2) / 2\n",
    "        if score > self.max_score:\n",
    "            self.max_score = score\n",
    "            self.best_option = self.scored_model_ind\n",
    "        scored_model = {}\n",
    "        scored_model['score'] = score\n",
    "        scored_model['lost_true_values'] = lost_true_values\n",
    "        scored_model['lost_true_values_percentage'] = lost_true_values_percentage\n",
    "        scored_model['false_positives'] = false_positives\n",
    "        scored_model['false_positives_percentage'] = false_positives_percentage\n",
    "        scored_model['false_negatives'] = false_negatives\n",
    "        scored_model['false_negatives_percentage'] = false_negatives_percentage\n",
    "        scored_model['lower_threshold'] = lower_threshold\n",
    "        scored_model['upper_threshold'] = upper_threshold\n",
    "        scored_model['predictions'] = predictions\n",
    "        self.scored_models.append(scored_model)\n",
    "        self.scored_model_ind += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Need special handling of the case number of FP == 0. Currently fails in this scenario.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Found 2400 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drevital/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:2006: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 14s 174ms/step - loss: 0.3876 - auc: 0.9982 - recall: 0.9975 - specifity: 0.9992\n",
      "loss :  0.3875793516635895\n",
      "auc :  0.9982079267501831\n",
      "recall :  0.9975000023841858\n",
      "specifity :  0.9991666674613953\n",
      "Found 2400 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drevital/anaconda3/lib/python3.7/site-packages/keras/engine/training.py:2035: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 13s 172ms/step\n",
      "\n",
      "No Obstacles Detected (True Negatives): 1161 (48.38%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 18 (0.75%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1191 (49.62%)\n",
      "Not Decided: 28 (1.17%)\n",
      "True Not Decided: 21 (0.88%)\n",
      "Total Obstacles: 2400\n",
      "Experimenting best max_ratio ...\n",
      "0.0050...\n",
      "Max Lost Ratio: 0.005\n",
      "No Obstacles Detected (True Negatives): 1162 (48.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 9 (0.38%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1180 (49.17%)\n",
      "Not Decided: 47 (1.96%)\n",
      "True Not Decided: 29 (1.22%)\n",
      "Total Obstacles: 2400\n",
      "0.0100...\n",
      "Max Lost Ratio: 0.01\n",
      "No Obstacles Detected (True Negatives): 1162 (48.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 9 (0.38%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1180 (49.17%)\n",
      "Not Decided: 47 (1.96%)\n",
      "True Not Decided: 29 (1.22%)\n",
      "Total Obstacles: 2400\n",
      "0.0150...\n",
      "Max Lost Ratio: 0.015\n",
      "No Obstacles Detected (True Negatives): 1151 (47.96%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 5 (0.21%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1167 (48.62%)\n",
      "Not Decided: 75 (3.12%)\n",
      "True Not Decided: 53 (2.23%)\n",
      "Total Obstacles: 2400\n",
      "0.0200...\n",
      "Max Lost Ratio: 0.02\n",
      "No Obstacles Detected (True Negatives): 1151 (47.96%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 5 (0.21%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1167 (48.62%)\n",
      "Not Decided: 75 (3.12%)\n",
      "True Not Decided: 53 (2.23%)\n",
      "Total Obstacles: 2400\n",
      "0.0250...\n",
      "Max Lost Ratio: 0.025\n",
      "No Obstacles Detected (True Negatives): 1151 (47.96%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 5 (0.21%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1167 (48.62%)\n",
      "Not Decided: 75 (3.12%)\n",
      "True Not Decided: 53 (2.23%)\n",
      "Total Obstacles: 2400\n",
      "0.0300...\n",
      "Max Lost Ratio: 0.030000000000000002\n",
      "No Obstacles Detected (True Negatives): 1129 (47.04%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 5 (0.21%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1167 (48.62%)\n",
      "Not Decided: 97 (4.04%)\n",
      "True Not Decided: 75 (3.16%)\n",
      "Total Obstacles: 2400\n",
      "0.0350...\n",
      "Max Lost Ratio: 0.034999999999999996\n",
      "No Obstacles Detected (True Negatives): 1129 (47.04%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1154 (48.08%)\n",
      "Not Decided: 111 (4.62%)\n",
      "True Not Decided: 88 (3.71%)\n",
      "Total Obstacles: 2400\n",
      "0.0400...\n",
      "Max Lost Ratio: 0.04\n",
      "No Obstacles Detected (True Negatives): 1129 (47.04%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1141 (47.54%)\n",
      "Not Decided: 124 (5.17%)\n",
      "True Not Decided: 101 (4.26%)\n",
      "Total Obstacles: 2400\n",
      "0.0450...\n",
      "Max Lost Ratio: 0.045\n",
      "No Obstacles Detected (True Negatives): 1129 (47.04%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1141 (47.54%)\n",
      "Not Decided: 124 (5.17%)\n",
      "True Not Decided: 101 (4.26%)\n",
      "Total Obstacles: 2400\n",
      "0.0500...\n",
      "Max Lost Ratio: 0.049999999999999996\n",
      "No Obstacles Detected (True Negatives): 1107 (46.12%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1141 (47.54%)\n",
      "Not Decided: 146 (6.08%)\n",
      "True Not Decided: 123 (5.18%)\n",
      "Total Obstacles: 2400\n",
      "0.0550...\n",
      "Max Lost Ratio: 0.055\n",
      "No Obstacles Detected (True Negatives): 1107 (46.12%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1141 (47.54%)\n",
      "Not Decided: 146 (6.08%)\n",
      "True Not Decided: 123 (5.18%)\n",
      "Total Obstacles: 2400\n",
      "0.0600...\n",
      "Max Lost Ratio: 0.06\n",
      "No Obstacles Detected (True Negatives): 1107 (46.12%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1115 (46.46%)\n",
      "Not Decided: 172 (7.17%)\n",
      "True Not Decided: 149 (6.28%)\n",
      "Total Obstacles: 2400\n",
      "0.0650...\n",
      "Max Lost Ratio: 0.065\n",
      "No Obstacles Detected (True Negatives): 1107 (46.12%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1115 (46.46%)\n",
      "Not Decided: 172 (7.17%)\n",
      "True Not Decided: 149 (6.28%)\n",
      "Total Obstacles: 2400\n",
      "0.0700...\n",
      "Max Lost Ratio: 0.07\n",
      "No Obstacles Detected (True Negatives): 1084 (45.17%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1115 (46.46%)\n",
      "Not Decided: 195 (8.12%)\n",
      "True Not Decided: 172 (7.25%)\n",
      "Total Obstacles: 2400\n",
      "0.0750...\n",
      "Max Lost Ratio: 0.07500000000000001\n",
      "No Obstacles Detected (True Negatives): 1084 (45.17%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 4 (0.17%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1115 (46.46%)\n",
      "Not Decided: 195 (8.12%)\n",
      "True Not Decided: 172 (7.25%)\n",
      "Total Obstacles: 2400\n",
      "0.0800...\n",
      "Max Lost Ratio: 0.08\n",
      "No Obstacles Detected (True Negatives): 1084 (45.17%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 2 (0.08%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1089 (45.38%)\n",
      "Not Decided: 223 (9.29%)\n",
      "True Not Decided: 198 (8.34%)\n",
      "Total Obstacles: 2400\n",
      "0.0850...\n",
      "Max Lost Ratio: 0.085\n",
      "No Obstacles Detected (True Negatives): 1084 (45.17%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 2 (0.08%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1089 (45.38%)\n",
      "Not Decided: 223 (9.29%)\n",
      "True Not Decided: 198 (8.34%)\n",
      "Total Obstacles: 2400\n",
      "0.0900...\n",
      "Max Lost Ratio: 0.09000000000000001\n",
      "No Obstacles Detected (True Negatives): 1061 (44.21%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 2 (0.08%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1089 (45.38%)\n",
      "Not Decided: 246 (10.25%)\n",
      "True Not Decided: 221 (9.31%)\n",
      "Total Obstacles: 2400\n",
      "0.0950...\n",
      "Max Lost Ratio: 0.095\n",
      "No Obstacles Detected (True Negatives): 1061 (44.21%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 2 (0.08%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1089 (45.38%)\n",
      "Not Decided: 246 (10.25%)\n",
      "True Not Decided: 221 (9.31%)\n",
      "Total Obstacles: 2400\n",
      "0.1000...\n",
      "Max Lost Ratio: 0.1\n",
      "No Obstacles Detected (True Negatives): 1061 (44.21%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1076 (44.83%)\n",
      "Not Decided: 260 (10.83%)\n",
      "True Not Decided: 234 (9.86%)\n",
      "Total Obstacles: 2400\n",
      "0.1050...\n",
      "Max Lost Ratio: 0.10500000000000001\n",
      "No Obstacles Detected (True Negatives): 1061 (44.21%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1063 (44.29%)\n",
      "Not Decided: 273 (11.38%)\n",
      "True Not Decided: 247 (10.41%)\n",
      "Total Obstacles: 2400\n",
      "0.1100...\n",
      "Max Lost Ratio: 0.11\n",
      "No Obstacles Detected (True Negatives): 1039 (43.29%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1063 (44.29%)\n",
      "Not Decided: 295 (12.29%)\n",
      "True Not Decided: 269 (11.34%)\n",
      "Total Obstacles: 2400\n",
      "0.1150...\n",
      "Max Lost Ratio: 0.115\n",
      "No Obstacles Detected (True Negatives): 1039 (43.29%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1063 (44.29%)\n",
      "Not Decided: 295 (12.29%)\n",
      "True Not Decided: 269 (11.34%)\n",
      "Total Obstacles: 2400\n",
      "0.1200...\n",
      "Max Lost Ratio: 0.12000000000000001\n",
      "No Obstacles Detected (True Negatives): 1039 (43.29%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1063 (44.29%)\n",
      "Not Decided: 295 (12.29%)\n",
      "True Not Decided: 269 (11.34%)\n",
      "Total Obstacles: 2400\n",
      "0.1250...\n",
      "Max Lost Ratio: 0.125\n",
      "No Obstacles Detected (True Negatives): 1039 (43.29%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1037 (43.21%)\n",
      "Not Decided: 321 (13.38%)\n",
      "True Not Decided: 295 (12.43%)\n",
      "Total Obstacles: 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1300...\n",
      "Max Lost Ratio: 0.13\n",
      "No Obstacles Detected (True Negatives): 1017 (42.38%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1037 (43.21%)\n",
      "Not Decided: 343 (14.29%)\n",
      "True Not Decided: 317 (13.36%)\n",
      "Total Obstacles: 2400\n",
      "0.1350...\n",
      "Max Lost Ratio: 0.135\n",
      "No Obstacles Detected (True Negatives): 1017 (42.38%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1037 (43.21%)\n",
      "Not Decided: 343 (14.29%)\n",
      "True Not Decided: 317 (13.36%)\n",
      "Total Obstacles: 2400\n",
      "0.1400...\n",
      "Max Lost Ratio: 0.14\n",
      "No Obstacles Detected (True Negatives): 1017 (42.38%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1037 (43.21%)\n",
      "Not Decided: 343 (14.29%)\n",
      "True Not Decided: 317 (13.36%)\n",
      "Total Obstacles: 2400\n",
      "0.1450...\n",
      "Max Lost Ratio: 0.14500000000000002\n",
      "No Obstacles Detected (True Negatives): 994 (41.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1011 (42.12%)\n",
      "Not Decided: 392 (16.33%)\n",
      "True Not Decided: 366 (15.42%)\n",
      "Total Obstacles: 2400\n",
      "0.1500...\n",
      "Max Lost Ratio: 0.15\n",
      "No Obstacles Detected (True Negatives): 994 (41.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1011 (42.12%)\n",
      "Not Decided: 392 (16.33%)\n",
      "True Not Decided: 366 (15.42%)\n",
      "Total Obstacles: 2400\n",
      "0.1550...\n",
      "Max Lost Ratio: 0.155\n",
      "No Obstacles Detected (True Negatives): 994 (41.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1011 (42.12%)\n",
      "Not Decided: 392 (16.33%)\n",
      "True Not Decided: 366 (15.42%)\n",
      "Total Obstacles: 2400\n",
      "0.1600...\n",
      "Max Lost Ratio: 0.16\n",
      "No Obstacles Detected (True Negatives): 994 (41.42%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 1011 (42.12%)\n",
      "Not Decided: 392 (16.33%)\n",
      "True Not Decided: 366 (15.42%)\n",
      "Total Obstacles: 2400\n",
      "0.1650...\n",
      "Max Lost Ratio: 0.165\n",
      "No Obstacles Detected (True Negatives): 971 (40.46%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 998 (41.58%)\n",
      "Not Decided: 428 (17.83%)\n",
      "True Not Decided: 402 (16.94%)\n",
      "Total Obstacles: 2400\n",
      "0.1700...\n",
      "Max Lost Ratio: 0.17\n",
      "No Obstacles Detected (True Negatives): 971 (40.46%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 984 (41.00%)\n",
      "Not Decided: 442 (18.42%)\n",
      "True Not Decided: 416 (17.53%)\n",
      "Total Obstacles: 2400\n",
      "0.1750...\n",
      "Max Lost Ratio: 0.17500000000000002\n",
      "No Obstacles Detected (True Negatives): 971 (40.46%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 984 (41.00%)\n",
      "Not Decided: 442 (18.42%)\n",
      "True Not Decided: 416 (17.53%)\n",
      "Total Obstacles: 2400\n",
      "0.1800...\n",
      "Max Lost Ratio: 0.18000000000000002\n",
      "No Obstacles Detected (True Negatives): 971 (40.46%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 984 (41.00%)\n",
      "Not Decided: 442 (18.42%)\n",
      "True Not Decided: 416 (17.53%)\n",
      "Total Obstacles: 2400\n",
      "0.1850...\n",
      "Max Lost Ratio: 0.185\n",
      "No Obstacles Detected (True Negatives): 949 (39.54%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 984 (41.00%)\n",
      "Not Decided: 464 (19.33%)\n",
      "True Not Decided: 438 (18.46%)\n",
      "Total Obstacles: 2400\n",
      "0.1900...\n",
      "Max Lost Ratio: 0.19\n",
      "No Obstacles Detected (True Negatives): 949 (39.54%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 957 (39.88%)\n",
      "Not Decided: 491 (20.46%)\n",
      "True Not Decided: 465 (19.60%)\n",
      "Total Obstacles: 2400\n",
      "0.1950...\n",
      "Max Lost Ratio: 0.195\n",
      "No Obstacles Detected (True Negatives): 949 (39.54%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 957 (39.88%)\n",
      "Not Decided: 491 (20.46%)\n",
      "True Not Decided: 465 (19.60%)\n",
      "Total Obstacles: 2400\n",
      "0.2000...\n",
      "Max Lost Ratio: 0.2\n",
      "No Obstacles Detected (True Negatives): 949 (39.54%)\n",
      "No Obstacles Incorrectly Detected (False Positives): 1 (0.04%)\n",
      "Obstacles Missed (False Negatives): 2 (0.08%)\n",
      "Obstacles Detected (True Positives): 957 (39.88%)\n",
      "Not Decided: 491 (20.46%)\n",
      "True Not Decided: 465 (19.60%)\n",
      "Total Obstacles: 2400\n",
      "\n",
      "\n",
      "\n",
      "best option score: 0.9914000679787689\n",
      "best option lower threshold: 0.3818632960319519\n",
      "best option upper threshold: 0.8093013167381287\n",
      "best option lost true values: 29 (1.22%)\n",
      "best option false negatives: 2 (0.08%)\n",
      "best option false positives: 9 (0.38%)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LocalDatasetHandler' object has no attribute 'metrics_csv_writer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1c4ca0072ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Retrieve model's metrics from SageMaker and write them to the handler's .csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m handler.write_metrics_to_csv(trial_component_display_name,\n\u001b[0;32m--> 106\u001b[0;31m                              print_params=print_best_option_params)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-af7027674498>\u001b[0m in \u001b[0;36mwrite_metrics_to_csv\u001b[0;34m(self, trial_component_display_name, print_params)\u001b[0m\n\u001b[1;32m    575\u001b[0m                     \u001b[0mmetrics_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_csv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;31m# Method to run when class is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LocalDatasetHandler' object has no attribute 'metrics_csv_writer'"
     ]
    }
   ],
   "source": [
    "# -- Variables --\n",
    "trial_component_display_name = 'gg-2-const-2021-08-25-14-17-49-training-trial'\n",
    "model_path = '/home/drevital/cs_video_processor/models/new_factory_gg_2_const'\n",
    "cloud_dataset = False\n",
    "obstacle_dataset = '/home/drevital/obstacles_classification_datasets/new_factory_gg_2_const/eval/obstacle'\n",
    "no_obstacle_dataset = '/home/drevital/obstacles_classification_datasets/new_factory_gg_2_const/eval/no_obstacle'\n",
    "model_name = model_path.split('/')[-1]\n",
    "color_mode = 'rgb'\n",
    "#thresh_csv_fname = f'model_thresholds.{model_name}.csv'\n",
    "#metrics_csv_fname = f'model_metrics.{model_name}.csv'\n",
    "thresh_csv_fname = None\n",
    "metrics_csv_fname = None\n",
    "rand_images = False\n",
    "num_rand_images = 10\n",
    "print_cm_params=True\n",
    "display_cm_figure = False\n",
    "display_false_negatives = False\n",
    "display_false_positives = False\n",
    "display_not_decided = False\n",
    "print_best_option_params=False\n",
    "\n",
    "# -- Run the Evaluation --\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "dataset_handlers = [LocalDatasetHandler, CloudDatasetHandler]\n",
    "handler = dataset_handlers[cloud_dataset](model_path,\n",
    "                                          obstacle_dataset,\n",
    "                                          no_obstacle_dataset,\n",
    "                                          rand_images=rand_images,\n",
    "                                          num_rand_images=num_rand_images,\n",
    "                                          thresh_csv_fname=thresh_csv_fname,\n",
    "                                          metrics_csv_fname=metrics_csv_fname)\n",
    "\n",
    "# -- Print metrics\n",
    "metrics = handler.print_model_metrics(model, color_mode)\n",
    "labels = np.array([0]*(handler.num_no_obstacles) + [1]*(handler.num_obstacles))\n",
    "\n",
    "# -- Predict with the model\n",
    "predictions, filenames = handler.get_predictions(model, color_mode=color_mode)\n",
    "\n",
    "# -- Print confusion-matrix considering the older \"no decision\" thresholds\n",
    "threshold = 0.5\n",
    "decision_margin = 0.25\n",
    "lower_threshold = threshold * (1.00 - decision_margin)\n",
    "upper_threshold = threshold + decision_margin * (1.00 - threshold)\n",
    "handler.calc_cm_considering_thresholds(model_path,\n",
    "                                       labels,\n",
    "                                       predictions,\n",
    "                                       lower_threshold,\n",
    "                                       upper_threshold,\n",
    "                                       print_params=print_cm_params,\n",
    "                                       plot=display_cm_figure,\n",
    "                                       threshold=threshold)\n",
    "\n",
    "# Find optimal lower, upper thresholds per given max limit of \"not decided\" (\"lost\") predictions\n",
    "max_lost_ratios = np.arange(start=0.005, stop=0.205, step=0.005)\n",
    "\n",
    "print('Experimenting best max_ratio ...')\n",
    "for max_lost_ratio in max_lost_ratios:\n",
    "    lower_threshold, upper_threshold = handler.find_thresholds(predictions,\n",
    "                                                               max_lost_ratio=max_lost_ratio)\n",
    "    print(f'{max_lost_ratio:.4f}', end='...')\n",
    "    # Print confusion-matrix after using recommended thresholds\n",
    "    handler.calc_cm_considering_thresholds(model_path,\n",
    "                                           labels,\n",
    "                                           predictions,\n",
    "                                           lower_threshold,\n",
    "                                           upper_threshold,\n",
    "                                           max_lost_ratio=max_lost_ratio,\n",
    "                                           threshold=0.5,\n",
    "                                           print_params=print_cm_params,\n",
    "                                           plot=display_cm_figure,\n",
    "                                           print_to_csv=False)\n",
    "    \n",
    "best_option = handler.model_score.scored_models[handler.model_score.best_option]\n",
    "\n",
    "print('\\n\\n')\n",
    "print(f'best option score: {best_option[\"score\"]}')\n",
    "print(f'best option lower threshold: {best_option[\"lower_threshold\"]}')\n",
    "print(f'best option upper threshold: {best_option[\"upper_threshold\"]}')\n",
    "print(f'best option lost true values: {best_option[\"lost_true_values\"]}', end = ' ')\n",
    "print(f'({best_option[\"lost_true_values_percentage\"]*100:.2f}%)')\n",
    "print(f'best option false negatives: {best_option[\"false_negatives\"]}', end=' ')\n",
    "print(f'({best_option[\"false_negatives_percentage\"]*100:.2f}%)')\n",
    "print(f'best option false positives: {best_option[\"false_positives\"]}', end=' ')\n",
    "print(f'({best_option[\"false_positives_percentage\"]*100:.2f}%)')\n",
    "\n",
    "lower_threshold = best_option['lower_threshold']\n",
    "upper_threshold = best_option['upper_threshold']\n",
    "    \n",
    "if display_false_negatives:\n",
    "    print('\\nFALSE NEGATIVES\\n')\n",
    "    handler.display_false_negatives(predictions, filenames, lower_threshold)\n",
    "\n",
    "if display_false_positives:\n",
    "    print('\\nFALSE POSITIVES\\n')\n",
    "    handler.display_false_positives(predictions, filenames, upper_threshold)\n",
    "\n",
    "if display_not_decided:\n",
    "    print('\\nNOT DECIDED\\n')\n",
    "    handler.display_not_decided(predictions, filenames, lower_threshold, upper_threshold)\n",
    "\n",
    "print()\n",
    "    \n",
    "# Retrieve model's metrics from SageMaker and write them to the handler's .csv file\n",
    "handler.write_metrics_to_csv(trial_component_display_name,\n",
    "                             print_params=print_best_option_params)\n",
    "del handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
