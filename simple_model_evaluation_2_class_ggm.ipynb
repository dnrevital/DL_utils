{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-05T09:05:42.307352Z",
     "iopub.status.busy": "2021-07-05T09:05:42.306996Z",
     "iopub.status.idle": "2021-07-05T09:05:42.315414Z",
     "shell.execute_reply": "2021-07-05T09:05:42.314278Z",
     "shell.execute_reply.started": "2021-07-05T09:05:42.307324Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import DirectoryIterator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import seaborn as sns\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes to handle dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 dataset,\n",
    "                 img_width=600,\n",
    "                 img_height=200,\n",
    "                 batch_size=32):\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self.dataset = dataset\n",
    "        self.obstacle_dataset = os.path.join(dataset, 'obstacle')\n",
    "        self.no_obstacle_dataset = os.path.join(dataset, 'no_obstacle')\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.obstacle_images = []\n",
    "        self.no_obstacle_images = []\n",
    "        self.sdv_images = []\n",
    "        self._update_image_lists = False\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        # Will be determined while reading all images from dataset\n",
    "        self.num_obstacles = 0\n",
    "        self.num_no_obstacles = 0\n",
    "        self.num_sdvs = 0\n",
    "        self.num_images = 0\n",
    "            \n",
    "        (self.obstacle_image_names,\n",
    "        self.num_obstacles) = self._get_all_dataset_image_names(self.dataset, 'obstacle')\n",
    "        (self.no_obstacle_image_names,\n",
    "        self.num_no_obstacles) = self._get_all_dataset_image_names(self.dataset, 'no_obstacle')\n",
    "        self.datagen, self.steps = self.get_datagen(self.batch_size)\n",
    "                            \n",
    "    def _get_all_dataset_image_names(self, dataset, class_name):\n",
    "        class_dataset = os.path.join(dataset, class_name)\n",
    "        image_names = os.listdir(class_dataset)\n",
    "        image_paths = [os.path.join(class_dataset, image_name) for image_name in image_names]\n",
    "        \n",
    "        return image_paths, len(image_paths)\n",
    "            \n",
    "    def _get_image(self, imname):\n",
    "        imrgb = cv2.imread(imname)\n",
    "        im = cv2.cvtColor(imrgb, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def get_datagen(self, batch_size):\n",
    "        datagen = ImageDataGenerator(rescale=1./255)\n",
    "        it = DirectoryIterator(self.dataset, image_data_generator=datagen, batch_size=batch_size)\n",
    "        steps= it.__len__()\n",
    "        \n",
    "        return datagen, steps\n",
    "\n",
    "    # Special generator to generate the 3 parts of the input image as 3 separate input images\n",
    "    def three_im_generator(self, gen, dataset, target_size, batch_size, class_mode):\n",
    "\n",
    "        im_gen = gen.flow_from_directory(dataset, \n",
    "                                         target_size=target_size, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         color_mode='grayscale',\n",
    "                                         class_mode=class_mode)\n",
    "        self.filenames = im_gen.filenames\n",
    "\n",
    "        while True:\n",
    "            im1_s, im2_s, im3_s = [], [], []\n",
    "            images, labels = im_gen.next()\n",
    "\n",
    "            for im in images:\n",
    "                w = im.shape[1]\n",
    "                im1 = im[:, :w//3]\n",
    "                im2 = im[:, w//3:(w*2)//3] \n",
    "                im3 = im[:, (w*2)//3:] \n",
    "                im1_s.append(im1)\n",
    "                im2_s.append(im2)\n",
    "                im3_s.append(im3)\n",
    "\n",
    "            im1_s = np.array(im1_s)\n",
    "            im2_s = np.array(im2_s)\n",
    "            im3_s = np.array(im3_s)\n",
    "            yield [im1_s, im2_s, im3_s], labels\n",
    "            \n",
    "    def get_metrics(self, model, color_mode='rgb'):\n",
    "        eval_generator = self.three_im_generator(\n",
    "            self.datagen,\n",
    "            self.dataset,\n",
    "            target_size=(self.img_height, self.img_width),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        \n",
    "        return model.evaluate(eval_generator, steps=self.steps, verbose=1)\n",
    "        \n",
    "    def get_predictions(self, model, color_mode='rgb'):\n",
    "        predict_generator = self.three_im_generator(\n",
    "            self.datagen,\n",
    "            self.dataset,\n",
    "            target_size=(self.img_height, self.img_width),\n",
    "            batch_size=self.batch_size,\n",
    "            class_mode='binary'\n",
    "        )\n",
    "        \n",
    "        return model.predict(predict_generator, steps=self.steps, verbose=1)\n",
    "        \n",
    "    def print_model_metrics(self, model, color_mode):\n",
    "        metrics = self.get_metrics(model, color_mode)\n",
    "\n",
    "        for name, value in zip(model.metrics_names, metrics):\n",
    "            print(name, ': ', value)\n",
    "            \n",
    "    # Compute and plot multi-class confusion-matrix\n",
    "    def plot_cm(self, model_path, labels, predictions, threshold):\n",
    "        cm = confusion_matrix(labels, predictions > threshold) \n",
    "        print(f'cm: {cm}')\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Threshold = {threshold}')\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label') \n",
    "        \n",
    "    # Compute and plot multi-class confusion-matrix with normalization\n",
    "    def plot_cm_normalized(self, model_path, labels, predictions, threshold):\n",
    "        cm = confusion_matrix(labels, predictions > threshold)\n",
    "        print(f'cm: {cm}')\n",
    "        # Normalise\n",
    "        cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        sns.heatmap(cmn, annot=True, fmt='.3f')\n",
    "        plt.title(f'Threshold = {threshold}')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show(block=False)  \n",
    "                \n",
    "    def save_false_positives(self, predictions, threshold, save_path):\n",
    "        false_positives = [fname for i, fname in enumerate(self.filenames[:self.num_no_obstacles])\\\n",
    "                           if predictions[i] > threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[:self.num_no_obstacles]) if p > threshold]\n",
    "\n",
    "        fp_path = os.path.join(save_path, 'false_positives')\n",
    "        Path(fp_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(false_positives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.no_obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fp_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "            \n",
    "    def save_true_positives(self, predictions, threshold, save_path):\n",
    "        true_positives = [fname for i, fname in enumerate(self.filenames[self.num_no_obstacles:])\\\n",
    "                           if predictions[self.num_no_obstacles+i] > threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[self.num_no_obstacles:]) if p > threshold]\n",
    "\n",
    "        fp_path = os.path.join(save_path, 'true_positives')\n",
    "        Path(fp_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(true_positives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fp_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "\n",
    "    def save_false_negatives(self, predictions, threshold, save_path):\n",
    "        false_negatives = [fname for i, fname in enumerate(self.filenames[self.num_no_obstacles:])\\\n",
    "                           if predictions[self.num_no_obstacles+i] <= threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[self.num_no_obstacles:]) if p <= threshold]\n",
    "        \n",
    "        fn_path = os.path.join(save_path, 'false_negatives')\n",
    "        Path(fn_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(false_negatives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fn_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "            \n",
    "    def save_true_negatives(self, predictions, threshold, save_path):\n",
    "        true_negatives = [fname for i, fname in enumerate(self.filenames[:self.num_no_obstacles])\\\n",
    "                           if predictions[i] <= threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[:self.num_no_obstacles]) if p <= threshold]\n",
    "        \n",
    "        fn_path = os.path.join(save_path, 'true_negatives')\n",
    "        Path(fn_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(true_negatives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.no_obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fn_path, imname)\n",
    "            cv2.imwrite(out_path, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Found 1931 images belonging to 2 classes.\n",
      "Found 1931 images belonging to 2 classes.\n",
      "61/61 [==============================] - 20s 317ms/step\n",
      "cm: [[ 787   10]\n",
      " [  30 1104]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Variables --\n",
    "model_path = '/home/drevital/cs_video_processor/models/humid_feb22_ggm_2'\n",
    "dataset = '/home/drevital/obstacles_classification_datasets/humid_feb22_ggm_2/eval'\n",
    "model_name = model_path.split('/')[-1]\n",
    "color_mode = 'grayscale'\n",
    "thresholds = [0.5]\n",
    "batch_size = 32\n",
    "save_base_path = '/home/drevital/obstacles_classification_datasets/model_eval'\n",
    "\n",
    "# -- Run the Evaluation --\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "handler = DatasetHandler(model_path, dataset, batch_size=batch_size)\n",
    "\n",
    "# -- Define Labels\n",
    "labels = np.array([0]*handler.num_no_obstacles\\\n",
    "                + [1]*handler.num_obstacles)\n",
    "\n",
    "# -- Predict with the model\n",
    "predictions = handler.get_predictions(model, color_mode=color_mode)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # -- Print confision-matrix\n",
    "    handler.plot_cm_normalized(model_path, labels, predictions, threshold=threshold)\n",
    "\n",
    "    # -- Save Images\n",
    "    save_name = dataset.split('/')[-2] + '_' + str(threshold)\n",
    "    save_path = os.path.join(save_base_path, save_name)\n",
    "    handler.save_false_negatives(predictions, threshold=threshold, save_path=save_path)\n",
    "    handler.save_false_positives(predictions, threshold=threshold, save_path=save_path)\n",
    "    handler.save_true_negatives(predictions, threshold=threshold, save_path=save_path)\n",
    "    handler.save_true_positives(predictions, threshold=threshold, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
