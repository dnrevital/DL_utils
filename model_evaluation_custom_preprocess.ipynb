{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-07-05T09:05:42.307352Z",
     "iopub.status.busy": "2021-07-05T09:05:42.306996Z",
     "iopub.status.idle": "2021-07-05T09:05:42.315414Z",
     "shell.execute_reply": "2021-07-05T09:05:42.314278Z",
     "shell.execute_reply.started": "2021-07-05T09:05:42.307324Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import DirectoryIterator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import os\n",
    "from pathlib import Path\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "from numpy import argmax\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Custom Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self,\n",
    "                 obs_dir,\n",
    "                 no_obs_dir,\n",
    "                 batch_size=32,\n",
    "                 input_size=(200, 200),\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.shuffle = shuffle\n",
    "        obs_imnames = os.listdir(obs_dir)\n",
    "        no_obs_imnames = os.listdir(no_obs_dir)\n",
    "        self.obs = [[os.path.join(obs_dir, name), 1] for name in obs_imnames]\n",
    "        self.no_obs = [[os.path.join(no_obs_dir, name), 0] for name in no_obs_imnames]\n",
    "        self.items = self.obs + self.no_obs\n",
    "        self.filenames = [item[0] for item in self.items]\n",
    "        self.labels = [item[1] for item in self.items]\n",
    "        self.n = len(self.items)\n",
    "        self.steps = math.ceil(self.n/self.batch_size)\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.items)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        start = index * self.batch_size\n",
    "        end = min((index+1) * self.batch_size, self.n)\n",
    "        items = self.items[start:end]\n",
    "        im1_s, im2_s, im3_s, labels = [], [], [], []\n",
    "\n",
    "        for item in items:\n",
    "            im = cv2.imread(item[0])\n",
    "            imarr = np.array(im, dtype='float32')\n",
    "            w = imarr.shape[1]\n",
    "\n",
    "            im1 = imarr[:, :w//3]\n",
    "            im2 = imarr[:, w//3:(w*2)//3] \n",
    "            im3 = imarr[:, (w*2)//3:] \n",
    "\n",
    "            im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)\n",
    "            im2 = cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)\n",
    "            im3 = cv2.cvtColor(im3, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            im1 = cv2.resize(im1, dsize=self.input_size, interpolation=cv2.INTER_LINEAR)\n",
    "            im2 = cv2.resize(im2, dsize=self.input_size, interpolation=cv2.INTER_LINEAR)\n",
    "            im3 = cv2.resize(im3, dsize=self.input_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            im1 /= 255.0\n",
    "            im2 /= 255.0\n",
    "            im3 /= 255.0\n",
    "\n",
    "            im1_s.append(im1)\n",
    "            im2_s.append(im2)\n",
    "            im3_s.append(im3)\n",
    "            labels.append(item[1])\n",
    "\n",
    "        im1_s = np.array(im1_s)\n",
    "        im2_s = np.array(im2_s)\n",
    "        im3_s = np.array(im3_s)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return [im1_s, im2_s, im3_s], labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes to handle dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetHandler:\n",
    "    def __init__(self,\n",
    "                 model_path,\n",
    "                 dataset,\n",
    "                 img_width=600,\n",
    "                 img_height=200,\n",
    "                 batch_size=32):\n",
    "        \n",
    "        self.model_path = model_path\n",
    "        self.dataset = dataset\n",
    "        self.obstacle_dataset = os.path.join(dataset, 'obstacle')\n",
    "        self.no_obstacle_dataset = os.path.join(dataset, 'no_obstacle')\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.obstacle_images = []\n",
    "        self.no_obstacle_images = []\n",
    "        self.sdv_images = []\n",
    "        self._update_image_lists = False\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        # Will be determined while reading all images from dataset\n",
    "        self.num_obstacles = 0\n",
    "        self.num_no_obstacles = 0\n",
    "        self.num_sdvs = 0\n",
    "        self.num_images = 0\n",
    "            \n",
    "        (self.obstacle_image_names,\n",
    "        self.num_obstacles) = self._get_all_dataset_image_names(self.dataset, 'obstacle')\n",
    "        (self.no_obstacle_image_names,\n",
    "        self.num_no_obstacles) = self._get_all_dataset_image_names(self.dataset, 'no_obstacle')\n",
    "        self.datagen, self.steps = self.get_datagen(self.batch_size)\n",
    "                            \n",
    "    def _get_all_dataset_image_names(self, dataset, class_name):\n",
    "        class_dataset = os.path.join(dataset, class_name)\n",
    "        image_names = os.listdir(class_dataset)\n",
    "        image_paths = [os.path.join(class_dataset, image_name) for image_name in image_names]\n",
    "        \n",
    "        return image_paths, len(image_paths)\n",
    "            \n",
    "    def _get_image(self, imname):\n",
    "        imrgb = cv2.imread(imname)\n",
    "        im = cv2.cvtColor(imrgb, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "        return im\n",
    "    \n",
    "    def get_datagen(self, batch_size):\n",
    "        datagen = ImageDataGenerator(rescale=1./255.)\n",
    "        it = DirectoryIterator(self.dataset, image_data_generator=datagen, batch_size=batch_size)\n",
    "        steps= it.__len__()\n",
    "        \n",
    "        return datagen, steps\n",
    " \n",
    "    def get_metrics(self, model, color_mode='rgb'):\n",
    "        eval_generator = self.three_im_generator_common_size(\n",
    "            self.datagen,\n",
    "            self.dataset,\n",
    "            target_size=(self.img_height, self.img_width),\n",
    "            batch_size=self.batch_size,\n",
    "            #class_mode='binary'\n",
    "            class_mode='categorical'\n",
    "        )\n",
    "        \n",
    "        return model.evaluate(eval_generator, steps=self.steps, verbose=1)\n",
    "        \n",
    "    def get_predictions(self, model, color_mode='rgb'):\n",
    "        predict_generator = CustomDataGen(self.obstacle_dataset,\n",
    "                                          self.no_obstacle_dataset,\n",
    "                                          shuffle=False)\n",
    "        self.filenames = predict_generator.filenames\n",
    "        self.labels = predict_generator.labels\n",
    "        self.items = predict_generator.items\n",
    "        return model.predict(predict_generator, steps=self.steps, verbose=1)\n",
    "        \n",
    "    def print_model_metrics(self, model, color_mode):\n",
    "        metrics = self.get_metrics(model, color_mode)\n",
    "\n",
    "        for name, value in zip(model.metrics_names, metrics):\n",
    "            print(name, ': ', value)\n",
    "            \n",
    "    # Compute and plot multi-class confusion-matrix\n",
    "    def plot_cm(self, model_path, labels, predictions, threshold):\n",
    "        cm = confusion_matrix(labels, predictions > threshold) \n",
    "        print(f'cm: {cm}')\n",
    "        sns.heatmap(cm, annot=True, fmt='d')\n",
    "        plt.title(f'Threshold = {threshold}')\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label') \n",
    "        \n",
    "    # Compute and plot multi-class confusion-matrix with normalization\n",
    "    def plot_cm_normalized(self, model_path, labels, predictions, threshold):\n",
    "        cm = confusion_matrix(labels, predictions > threshold)\n",
    "        print(f'cm: {cm}')\n",
    "        # Normalise\n",
    "        cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "        sns.heatmap(cmn, annot=True, fmt='.3f')\n",
    "        plt.title(f'Threshold = {threshold}')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.show(block=False)  \n",
    "                \n",
    "    def save_false_positives(self, predictions, threshold, save_path):\n",
    "        false_positives = [fname for i, fname in enumerate(self.filenames[self.num_obstacles:])\\\n",
    "                           if predictions[self.num_obstacles+i] > threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[self.num_obstacles:]) if p > threshold]\n",
    "\n",
    "        fp_path = os.path.join(save_path, 'false_positives')\n",
    "        Path(fp_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(false_positives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.no_obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fp_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "            \n",
    "        # Return the prediction as a list for histogram plot\n",
    "        fp_preds = [p[0] for p in preds]\n",
    "        return fp_preds\n",
    "            \n",
    "    def save_true_positives(self, predictions, threshold, save_path):\n",
    "        true_positives = [fname for i, fname in enumerate(self.filenames[:self.num_obstacles])\\\n",
    "                           if predictions[i] > threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[:self.num_obstacles]) if p > threshold]\n",
    "\n",
    "        tp_path = os.path.join(save_path, 'true_positives')\n",
    "        Path(tp_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(true_positives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(tp_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "\n",
    "    def save_false_negatives(self, predictions, threshold, save_path):\n",
    "        false_negatives = [fname for i, fname in enumerate(self.filenames[:self.num_obstacles])\\\n",
    "                           if predictions[i] <= threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[:self.num_obstacles]) if p <= threshold]\n",
    "        \n",
    "        fn_path = os.path.join(save_path, 'false_negatives')\n",
    "        Path(fn_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(false_negatives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(fn_path, imname)\n",
    "            cv2.imwrite(out_path, im)\n",
    "            \n",
    "        # Return the prediction as a list for histogram plot\n",
    "        fn_preds = [p[0] for p in preds]\n",
    "        return fn_preds\n",
    "            \n",
    "    def save_true_negatives(self, predictions, threshold, save_path):\n",
    "        true_negatives = [fname for i, fname in enumerate(self.filenames[self.num_obstacles:])\\\n",
    "                           if predictions[self.num_obstacles+i] <= threshold]\n",
    "        preds = [p for i, p in enumerate(predictions[self.num_obstacles:]) if p <= threshold]\n",
    "        \n",
    "        tn_path = os.path.join(save_path, 'true_negatives')\n",
    "        Path(tn_path).mkdir(parents=True, exist_ok=True)\n",
    "        for i, fname in enumerate(true_negatives):\n",
    "            imname = fname.split('/')[-1]\n",
    "            in_path = os.path.join(self.no_obstacle_dataset, imname)\n",
    "            im = cv2.imread(in_path)\n",
    "            imname = imname.split('_')\n",
    "            imname.insert(2, f'{preds[i][0]:.2f}')\n",
    "            imname = '_'.join(imname)\n",
    "            out_path = os.path.join(tn_path, imname)\n",
    "            cv2.imwrite(out_path, im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n",
      "Found 2430 images belonging to 2 classes.\n",
      "76/76 [==============================] - 35s 459ms/step\n",
      "cm: [[1179   25]\n",
      " [  35 1191]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -- Variables --\n",
    "model_path = '/home/drevital/cs_video_processor/models/jun22_f'\n",
    "dataset = '/home/drevital/obstacles_classification_datasets/jun22_f/eval'\n",
    "model_name = model_path.split('/')[-1]\n",
    "color_mode = 'rgb'\n",
    "thresholds = [0.75]\n",
    "batch_size = 32\n",
    "save_base_path = '/home/drevital/obstacles_classification_datasets/model_eval_new_method'\n",
    "\n",
    "# -- Run the Evaluation --\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "handler = DatasetHandler(model_path, dataset, batch_size=batch_size)\n",
    "\n",
    "# -- Define Labels\n",
    "#labels = np.array([0]*handler.num_no_obstacles\\\n",
    "#                + [1]*handler.num_obstacles)\n",
    "\n",
    "# -- Predict with the model\n",
    "predictions = handler.get_predictions(model, color_mode=color_mode)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # -- Print confision-matrix\n",
    "    handler.plot_cm_normalized(model_path, handler.labels, predictions, threshold=threshold)\n",
    "\n",
    "    # -- Save Images\n",
    "    save_name = dataset.split('/')[-2] + '_' + str(threshold)\n",
    "    save_path = os.path.join(save_base_path, save_name)\n",
    "    fn_preds = handler.save_false_negatives(predictions, threshold=threshold, save_path=save_path)\n",
    "    fp_preds = handler.save_false_positives(predictions, threshold=threshold, save_path=save_path)\n",
    "    handler.save_true_negatives(predictions, threshold=threshold, save_path=save_path)\n",
    "    handler.save_true_positives(predictions, threshold=threshold, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27951616],\n",
       "       [0.0089494 ],\n",
       "       [0.00174233],\n",
       "       [0.00439069],\n",
       "       [0.01581183],\n",
       "       [0.0470188 ],\n",
       "       [0.18177894],\n",
       "       [0.06037298],\n",
       "       [0.00715053],\n",
       "       [0.00354871]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
